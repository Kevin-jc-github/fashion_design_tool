import os
import numpy as np
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import argparse
import time
import json
from pathlib import Path

# é…ç½®å‚æ•°
MODEL_NAME = "openai/clip-vit-base-patch32"
IMAGE_DIR = "data/images/"  # å›¾ç‰‡åº“ç›®å½•
FEATURES_FILE = "data/image_features.json"  # ç‰¹å¾ç¼“å­˜æ–‡ä»¶

# ç¡®ä¿ç›®å½•å­˜åœ¨
Path(IMAGE_DIR).mkdir(parents=True, exist_ok=True)

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
print(f"â³ Loading CLIP model: {MODEL_NAME}...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained(MODEL_NAME).to(device)
processor = CLIPProcessor.from_pretrained(MODEL_NAME)
print(f"âœ… Model loaded on {device} device")

def precompute_image_features(image_dir, features_file):
    """é¢„è®¡ç®—å›¾ç‰‡åº“çš„ç‰¹å¾å‘é‡å¹¶å­˜å‚¨"""
    image_paths = []
    image_features = []
    
    print(f"ğŸ” Scanning image directory: {image_dir}")
    valid_extensions = ('.png', '.jpg', '.jpeg', '.webp')
    
    # å¦‚æœç‰¹å¾æ–‡ä»¶å­˜åœ¨ä¸”å›¾ç‰‡åº“æœªæ›´æ–°ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(features_file):
        with open(features_file, 'r') as f:
            cache = json.load(f)
            cache_mtime = cache.get("last_modified", 0)
            current_mtime = os.path.getmtime(image_dir)
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¿®æ”¹è¿‡
            if cache_mtime >= current_mtime and set(cache["image_paths"]) == set(
                [f for f in os.listdir(image_dir) if f.lower().endswith(valid_extensions)]
            ):
                print("â™»ï¸ Loading precomputed features from cache")
                return cache["image_paths"], np.array(cache["features"])
    
    # éœ€è¦é‡æ–°è®¡ç®—ç‰¹å¾
    print("ğŸ”„ Computing features for images...")
    start_time = time.time()
    
    for img_file in os.listdir(image_dir):
        if img_file.lower().endswith(valid_extensions):
            img_path = os.path.join(image_dir, img_file)
            try:
                image = Image.open(img_path)
                inputs = processor(images=image, return_tensors="pt").to(device)
                with torch.no_grad():
                    features = model.get_image_features(**inputs)
                features = features / features.norm(dim=-1, keepdim=True)  # å½’ä¸€åŒ–
                image_paths.append(img_path)
                image_features.append(features.cpu().numpy().tolist()[0])  # è½¬ä¸ºåˆ—è¡¨å­˜å‚¨
                print(f"  âœ” Processed: {img_file}")
            except Exception as e:
                print(f"  âŒ Error processing {img_file}: {e}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    image_features_np = np.array(image_features)
    
    # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
    cache_data = {
        "last_modified": os.path.getmtime(image_dir),
        "image_paths": [os.path.basename(p) for p in image_paths],
        "features": image_features_np.tolist()
    }
    
    with open(features_file, 'w') as f:
        json.dump(cache_data, f)
    
    print(f"ğŸ’¾ Saved features to {features_file}")
    print(f"â±ï¸ Feature computation took {time.time()-start_time:.2f} seconds")
    
    return image_paths, image_features_np

def text_search(query_text, all_image_paths, all_image_features, top_k=5):
    """æ–‡æœ¬æœç´¢å‡½æ•°"""
    print(f"\nğŸ” Searching for: '{query_text}'")
    start_time = time.time()
    
    # å¤„ç†æ–‡æœ¬
    inputs = processor(text=[query_text], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs)
    text_features = text_features / text_features.norm(dim=-1, keepdim=True)  # å½’ä¸€åŒ–
    text_features = text_features.cpu().numpy()

    # è®¡ç®—ç›¸ä¼¼åº¦ (ä½™å¼¦ç›¸ä¼¼åº¦)
    similarities = np.dot(text_features, all_image_features.T)[0]
    
    # è·å–ç›¸ä¼¼åº¦æœ€é«˜çš„top_kä¸ªç´¢å¼•
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    # æ‰“å°ç»“æœ
    print(f"ğŸ† Top {top_k} results:")
    results = []
    for rank, idx in enumerate(top_indices, 1):
        img_path = all_image_paths[idx]
        score = similarities[idx]
        print(f"  #{rank}: {img_path} - Similarity: {score:.4f}")
        results.append({"path": img_path, "score": score})
    
    print(f"â±ï¸ Search took {time.time()-start_time:.2f} seconds")
    return results

def image_search(query_image_path, all_image_paths, all_image_features, top_k=5):
    """å›¾æœå›¾å‡½æ•°"""
    print(f"\nğŸ–¼ï¸ Searching with image: {query_image_path}")
    start_time = time.time()
    
    try:
        # åŠ è½½å¹¶å¤„ç†æŸ¥è¯¢å›¾ç‰‡
        query_image = Image.open(query_image_path)
        inputs = processor(images=query_image, return_tensors="pt").to(device)
        with torch.no_grad():
            query_features = model.get_image_features(**inputs)
        query_features = query_features / query_features.norm(dim=-1, keepdim=True)
        query_features = query_features.cpu().numpy()

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(query_features, all_image_features.T)[0]
        
        # è·å–ç›¸ä¼¼åº¦æœ€é«˜çš„top_kä¸ªç´¢å¼•
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # æ‰“å°ç»“æœ
        print(f"ğŸ† Top {top_k} results:")
        results = []
        for rank, idx in enumerate(top_indices, 1):
            img_path = all_image_paths[idx]
            score = similarities[idx]
            print(f"  #{rank}: {img_path} - Similarity: {score:.4f}")
            results.append({"path": img_path, "score": score})
        
        print(f"â±ï¸ Search took {time.time()-start_time:.2f} seconds")
        return results
    except Exception as e:
        print(f"âŒ Error processing query image: {e}")
        return []

def print_help():
    """æ‰“å°å¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ¨ Fashion Visionary CLI Tool")
    print("Usage:")
    print("  Search by text: python search_cli.py --text \"design keywords\" [--topk 5]")
    print("  Search by image: python search_cli.py --image path/to/image.jpg [--topk 5]")
    print("  Update image library: python search_cli.py --update")
    print("\nExamples:")
    print("  python search_cli.py --text \"deconstructed futurism\"")
    print("  python search_cli.py --image data/images/texture_sample.jpg --topk 3")
    print("  python search_cli.py --update")

def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Fashion Visionary CLI Search Tool", add_help=False)
    parser.add_argument("--text", type=str, help="Text query for image search")
    parser.add_argument("--image", type=str, help="Path to query image for search")
    parser.add_argument("--topk", type=int, default=5, help="Number of top results to show (default: 5)")
    parser.add_argument("--update", action="store_true", help="Force update image features cache")
    parser.add_argument("--help", action="store_true", help="Show help message")
    
    args = parser.parse_args()
    
    if args.help:
        print_help()
        return
    
    # å¼ºåˆ¶æ›´æ–°ç¼“å­˜
    if args.update:
        if os.path.exists(FEATURES_FILE):
            os.remove(FEATURES_FILE)
            print("â™»ï¸ Feature cache cleared")
    
    # é¢„è®¡ç®—/åŠ è½½ç‰¹å¾
    print("\n" + "="*50)
    print("ğŸ“‚ Preparing image library...")
    all_image_paths, all_image_features = precompute_image_features(IMAGE_DIR, FEATURES_FILE)
    
    if not all_image_paths:
        print("âŒ No valid images found in library!")
        print("Please add images to data/images directory")
        return
    
    print(f"ğŸ“Š Library contains {len(all_image_paths)} images")
    print("="*50 + "\n")
    
    # æ‰§è¡Œæœç´¢
    if args.text:
        text_search(args.text, all_image_paths, all_image_features, args.topk)
    elif args.image:
        image_search(args.image, all_image_paths, all_image_features, args.topk)
    else:
        print("âš ï¸ Please provide a search query (--text or --image)")
        print("Use --help for usage instructions")

if __name__ == "__main__":
    main()